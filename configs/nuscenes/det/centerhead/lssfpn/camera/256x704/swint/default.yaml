data:
  val:
    type: NuScenesDataset
    dataset_root: ''
    ann_file: data/nuscenes/nuscenes_infos_val.pkl
  pipeline:
  - type: LoadMultiViewImageFromFiles
    to_float32: true
  - type: ImageAug3D
    final_dim: [256, 704]
    resize_lim: [0.0, 0.0]   # for test (no resize)
    bot_pct_lim: [0.0, 0.0]
    rot_lim: [0.0, 0.0]
    rand_flip: false
    is_train: false
  - type: GlobalRotScaleTrans
    resize_lim: [1.0, 1.0]
    rot_lim: [0.0, 0.0]
    trans_lim: 0.0
    is_train: false
  - type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  - type: DefaultFormatBundle3D
    classes:
      - car
      - truck
      - construction_vehicle
      - bus
      - trailer
      - barrier
      - motorcycle
      - bicycle
      - pedestrian
      - traffic_cone
  - 
    type: Collect3D
    keys: 
      - img
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - camera2lidar
      - img_aug_matrix
    modality:
      use_camera: true
      use_lidar: false
      use_radar: false
      use_map: false
      use_external: false
    test_mode: true
  samples_per_gpu: 1
model:
  type : BEVFusion
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [1, 2, 3]
        with_cp: false
        convert_weights: true
        init_cfg:
          type: Pretrained
          checkpoint: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
      neck:
        type: GeneralizedLSSFPN
        in_channels: [192, 384, 768]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false      
      vtransform:
        type: AwareBEVDepth
        in_channels: 256
        out_channels: 80
        image_size: [704, 256]         
        feature_size: [44, 16] 
        xbound: [-51.2, 51.2, 0.4]
        ybound: [-51.2, 51.2, 0.4]
        zbound: [-10.0, 10.0, 20.0]
        dbound: [1.0, 60.0, 0.5]
        downsample: 2
  decoder:
    backbone:
      type: GeneralizedResNet
      in_channels: 80
      blocks:
        - [2, 128, 2]
        - [2, 256, 2]
        - [2, 512, 1]
    neck:
      type: LSSFPN
      in_indices: [-1, 0]
      in_channels: [512, 128]
      out_channels: 256
      scale_factor: 2
  fuser:
    type: IdentityFuser  # camera only니까 실제로 fuse 안함
    in_channels: [256]  # vtransform output과 일치시켜야 함
    out_channels: 256

  heads:
    object:
      type: CenterHead  # 또는 기존 BEVFusion에서 쓰던 head
      in_channels: 256
      tasks:
        - num_class: 10  # 임시. class 개수에 따라 수정
          stride: 4
      common_heads:
        reg: [2, 2]
        height: [1, 2]
        dim: [3, 2]
        rot: [2, 2]
        vel: [2, 2]
      bbox_coder:
        type: CenterPointBBoxCoder
        pc_range: ${point_cloud_range}
        voxel_size: ${voxel_size} 
        post_center_range: [0, 0, 0, 0, 0, 0]
        max_num: 500
        score_threshold: 0.1
        out_size_factor: 4
optimizer:
  paramwise_cfg:
    custom_keys:
      absolute_pos_embed:
        decay_mult: 0
      relative_position_bias_table:
        decay_mult: 0
      encoders.camera.backbone:
        lr_mult: 0.1


lr_config:
  policy: cyclic
  target_ratio: 5.0
  cyclic_times: 1
  step_ratio_up: 0.4

momentum_config:
  policy: cyclic
  cyclic_times: 1
  step_ratio_up: 0.4
